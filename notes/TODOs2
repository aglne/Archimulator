Reading list:
    Predicting Inter-Thread Cache Contention on a Chip Multi-Processor Architecture
    Prefetching with Helper Threads for Loosely Coupled Multiprocessor Systems

Hints from Cache showndown: the good, bad and ugly:
    Useful prefetches are those that are actually accessed, while harmful prefetches are those that are never accessed
    and are a direct cause of misses to cache lines that were displaced by that prefetch line. Finally, useless
    prefetches are those that are never accessed.

    At any point in execution, the union of the non-prefetched tags in the cache and the evict tags in the ET give the state of of cache as if prefetching had never occurred.

TODOs:
    L2: 512K, 1M, 2M, 3M
    Benchmarks: mst, em3d, mcf

---------------------

Archimulator on the cloud:
    1. get CPU usage on Guest
    2. get Memory usage on Guest

TODOs:
    1. intra-thread and inter-thread stack distance and reuse distance profiling
    2. PC/load address relative + HT request taxonomy based auto-tuning parameters of the HT scheme
    2. viewing program output.. from UI

    *. a. late HT requests -> Spawn H/W thread to prefetch for HT
       b. bad HT requests -> Pause HT, resume HT in the next phase (128K insts in MT?)
       c. good HT requests -> increase/decrease confidence counter value

Paper hints:
    1. from "Scheduling Threads for Constructive Cache Sharing on CMPs":
       In contrast, our work focuses on promoting constructive cache interference by taking advantage
       of the potential overlap in memory references among cooperating threads that share an address space, particularly
       multithreaded programs, for the reasons cited above.

    2. from "PACMan: Prefetch-Aware Cache Management for High Performance Caching":
       Although prefetching improves performance by fetching useful data
       in advance, it can interact with LLC management policies to in-
       troduce application performance variability. This variability stems
       from the fact that current replacement policies treat prefetch and
       demand requests identically.
       ...
       PACMan dynamically estimates and mitigates the degree of prefetch-induced
       cache interference by modifying the cache insertion and hit pro-
       motion policies to treat demand and prefetch requests differently.
       ...
       While an intelligent cache management technique can improve application
       memory performance, prefetching can increase performance
       even more by fetching useful data in advance to the cache.
       However, we show that prefetching alone, without the help of
       a prefetch-aware cache management technique, can interact
       poorly with LLC management, often causing performance
       degradation and variability.
       ...
       In addition, we implement three recent replacement policy proposals: DRRIP [12],
       Segmented-LRU (Seg-LRU) [5], and Sampling Deadblock Prediction (SDBP) [16]
       and evaluate their performance in the presence of hardware prefetching.
       ...
       From a cache management perspective, prefetch requests have
       different properties than a demand request. In general, cache lines
       inserted into the LLC by demand requests are more likely to be
       performance-critical than prefetch requests. Thus, a replacement
       policy may benefit by giving more preference to items fetched by
       demand requests versus those from prefetch requests. This work
       explores different replacement policies for prefetch requests.
       ...
       Cache replacement policies essentially predict the re-reference
       behavior of cache lines. The natural opportunity to make re-
       reference predictions is on cache insertions and cache hits. For ex-
       ample, LRU predicts that a missing cache line will be re-referenced
       soon and always inserts the missing line at the “MRU position”
       of the LRU chain. Similarly, LRU predicts that a line receiving a
       cache hit will also be re-referenced soon and moves the line to the
       MRU position.
       ...
       We propose four types of prefetch-aware cache manage-
       ment policies:
       • PACMan on Misses (PACMan-M): PACMan-M modifies the
       re-reference prediction only when a prefetch request misses
       in the cache. In particular, PACMan-M predicts that all
       prefetch requests will be re-referenced in the distant future.
       For example, like previous LRU-based studies, PACMan-M
       can be implemented by inserting all prefetch requests at the
       LRU position [18, 31]. In doing so, PACMan-M reduces the
       average in-cache lifetime of a prefetched cache line, in ef-
       fect prioritizing demand requests over prefetch requests. In
       the event the line is subsequently re-referenced, the replace-
       ment state is updated on the cache hit. However, if there
       is no subsequent request, PACMan-M is useful for avoiding
       prefetcher pollution.
       • PACMan on Hits (PACMan-H): PACMan-H modifies the re-
       reference prediction only when a prefetch request hits in the
       cache. Unlike a conventional replacement policy that always
       updates re-reference predictions on cache hits, PACMan-H
       chooses not to update the re-reference prediction on prefetch
       cache hits. In doing so, PACMan-H treats “prefetchable” re-
       quests with lower priority. In doing so, PACMan-H enables
       useful, but hard to prefetch, demand requests to be retained
       in the cache.
       • PACMan on Hits and Misses (PACMan-HM): PACMan-HM
       modifies the re-reference predictions for prefetch request cache
       hits and misses. PACMan-HM gives shorter lifetime to all
       prefetch requests by using the PACMan-M policy on prefetch
       misses and the PACMan-H policy on prefetch hits. PACMan-
       HM is useful for avoiding prefetcher pollution and retaining
       hard to prefetch lines.
       • Dynamic PACMan (PACMan-DYN): Statically predicting
       the re-reference pattern of prefetch requests can significantly
       degrade performance of workloads that benefit from prefetch-
       ing. To ensure robust prefetch-aware replacement, we pro-
       pose to dynamically detect the re-reference behavior of
       prefetch requests. PACMan-DYN uses Set Dueling [27] to
       dynamically predict the re-reference behavior of prefetch re-
       quests.
       ...
       DRRIP Background
       Instead of using a 4-bit LRU counter per cache line for a 16-way
       set-associative cache, DRRIP uses an M-bit counter per cache line
       to store the lines re-reference prediction value (RRPV). The RRPV
       in essence controls the lifetime of a cache line. In general, large
       RRPVs cause insertion closer to the “LRU position” and, there-
       fore, have a short lifetime in the cache. In contrast, small RRPVs
       comparatively have a longer lifetime in the cache.

Arduino from Java:
-Djava.library.path="/home/itecgo/Tools/arduino-1.0/lib"

from Multicore-Aware Reuse Distance Analysis:
Definition of reuse distance in multithreaded program:
    For a multithreaded program, the reuse distance of a reference by
    thread T to address x is
    (a) ∞ if x has never been referenced before;
    (b) ∞ if x has been overwritten by a thread which does not
    share a cache with T since the last reference by a thread
    which shares a cache with T ; or
    (c) the number of distinct data accessed by threads which
    share a cache with T since the last reference to x by a
    thread which shares a cache with T .

from Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers :
The goal of this paper is to reduce the negative perfor-
mance and bandwidth impact of aggressive prefetching while
preserving the large performance benefits provided by it. To
achieve this goal, we propose simple and implementable mech-
anisms that dynamically adjust the aggressiveness of the hard-
ware prefetcher as well as the location in the processor cache
where prefetched data is inserted.
The proposed mechanisms estimate the effectiveness of the
prefetcher by monitoring the accuracy and timeliness of the
prefetch requests as well as the cache pollution caused by the
prefetch requests. We describe simple hardware implemen-
tations to estimate accuracy, timeliness, and cache pollution.
Based on the run-time estimation of these three metrics, the
aggressiveness of the hardware prefetcher is decreased or in-
creased dynamically. Also, based on the run-time estimation
of the cache pollution caused by the prefetcher, the proposed
mechanism dynamically decides where to insert the prefetched
blocks in the processor cache’s LRU stack.
Our results show that using the proposed dynamic feed-
back mechanisms improve the average performance of 17
memory-intensive benchmarks in the SPEC CPU2000 suite by
6.5% compared to the best-performing conventional stream-
based prefetcher configuration. With the proposed mechanism,
the negative performance impact incurred on some bench-
marks due to stream-based prefetching is completely elimi-
nated. Furthermore, the proposed mechanism consumes 18.7%
less memory bandwidth than the best-performing stream-based
prefetcher configuration. Compared to a conventional stream-
based prefetcher configuration that consumes similar amount
of memory bandwidth, feedback directed prefetching provides
13.6% higher performance. We also show that the dynamic
feedback mechanism works similarly well when implemented
to dynamically adjust the aggressiveness of a global-history-
buffer (GHB) based delta correlation prefetcher [10] or a
PC-based stride prefetcher [1]. Compared to a conventional
GHB-based delta correlation prefetcher configuration that con-
sumes similar amount of memory bandwidth, feedback di-
rected prefetching provides 9.9% higher performance. The
proposed mechanism provides these benefits with a modest
hardware storage cost of 2.54 KB and without significantly
increasing hardware complexity. On the remaining 9 SPEC
CPU2000 benchmarks, the proposed dynamic feedback mech-
anism performs as well as the best-performing conventional
stream prefetcher configuration for those 9 benchmarks.

Aggressiveness of the prefetcher is determined by how far the prefetcher
stays ahead of the demand access stream of the program as well as how many
prefetch requests are generated, as shown in Table 1 and Section 2.1.

http://code.google.com/p/cache-coherence-simulator/

add ratio to fu/... occupancies
read: A Hardware-based Cache Pollution Filtering Mechanism for Aggressive Prefetches

* em3d, mcf support (functional -> two-phase -> detailed)




